"""
Endpoints API para Search Service
"""

import logging
import time
import uuid
from datetime import datetime
from typing import List, Optional

from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Query
from fastapi.responses import JSONResponse
from models.schemas import (
    BulkIndexRequest,
    BulkIndexResponse,
    ErrorResponse,
    HealthCheck,
    Metrics,
    ProviderInfo,
    SearchMetadata,
    SearchRequest,
    SearchResult,
    SuggestionRequest,
    SuggestionResponse,
)
from services.cache_service import cache_service
from services.search_service import search_service
from utils.text_processor import analyze_query

from shared_lib.config import settings

router = APIRouter()
logger = logging.getLogger(__name__)


@router.post("/search", response_model=SearchResult)
async def search_providers(
    request: SearchRequest,
    background_tasks: BackgroundTasks,
    x_request_id: Optional[str] = None,
):
    """
    Buscar proveedores por texto libre

    - **query**: Texto de b√∫squeda (ej: "necesito m√©dico en Quito")
    - **filters**: Filtros opcionales (ciudad, rating, etc.)
    - **limit**: L√≠mite de resultados (default: 10)
    - **use_ai_enhancement**: Usar IA para mejorar b√∫squeda (default: true)
    """
    request_id = x_request_id or str(uuid.uuid4())

    try:
        logger.info(f"üîç B√∫squeda [{request_id}]: {request.query}")

        # Validar l√≠mite
        if request.limit > settings.max_search_results:
            raise HTTPException(
                status_code=400,
                detail=f"El l√≠mite no puede exceder {settings.max_search_results}",
            )

        # Ejecutar b√∫squeda
        result = await search_service.search_providers(request)

        # Log de resultados
        logger.info(
            f"‚úÖ B√∫squeda [{request_id}]: {len(result.providers)} resultados "
            f"({result.metadata.search_time_ms}ms, estrategia: {result.metadata.search_strategy})"
        )

        # Tarea en background para actualizar estad√≠sticas
        background_tasks.add_task(
            update_search_statistics, request_id, request.query, result.metadata
        )

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error en b√∫squeda [{request_id}]: {e}")
        raise HTTPException(
            status_code=500, detail="Error interno en el servicio de b√∫squeda"
        )


@router.get("/suggestions", response_model=SuggestionResponse)
async def get_suggestions(
    q: str = Query(..., min_length=1, max_length=100, description="Consulta parcial"),
    limit: int = Query(default=5, ge=1, le=20, description="L√≠mite de sugerencias"),
):
    """
    Obtener sugerencias de autocompletado

    - **q**: Texto parcial para autocompletar
    - **limit**: N√∫mero m√°ximo de sugerencias
    """
    try:
        if not q.strip():
            raise HTTPException(
                status_code=400, detail="La consulta no puede estar vac√≠a"
            )

        suggestions = await search_service.get_suggestions(q.strip(), limit)

        return SuggestionResponse(
            suggestions=suggestions,
            completions=[],  # TODO: Implementar completions
            corrections=[],  # TODO: Implementar correcciones
            metadata={
                "query": q,
                "suggestions_count": len(suggestions),
                "generated_at": datetime.now().isoformat(),
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error obteniendo sugerencias para '{q}': {e}")
        raise HTTPException(status_code=500, detail="Error obteniendo sugerencias")


@router.get("/analyze", response_model=dict)
async def analyze_query_endpoint(
    q: str = Query(..., min_length=1, max_length=500, description="Consulta a analizar")
):
    """
    Analizar una consulta para depuraci√≥n

    - **q**: Texto de consulta para analizar
    """
    try:
        analysis = analyze_query(q)
        return {
            "query": q,
            "analysis": analysis,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"Error analizando consulta '{q}': {e}")
        raise HTTPException(status_code=500, detail="Error analizando consulta")


@router.get("/health", response_model=HealthCheck)
async def health_check():
    """
    Verificar salud del servicio
    """
    try:
        # Obtener informaci√≥n de salud de los servicios
        search_health = await search_service.health_check()
        cache_info = await cache_service.get_cache_info()

        # Obtener m√©tricas b√°sicas
        metrics = await cache_service.get_metrics() or Metrics()

        return HealthCheck(
            status="healthy" if search_health["search_service_ready"] else "unhealthy",
            timestamp=datetime.now(),
            version="1.0.0",
            database_connected=search_health["database_connected"],
            redis_connected=cache_info.get("connected", False),
            search_service_ready=search_health["search_service_ready"],
            uptime_seconds=0,  # TODO: Implementar uptime tracking
            metrics=metrics,
        )

    except Exception as e:
        logger.error(f"Error en health check: {e}")
        return HealthCheck(
            status="unhealthy",
            timestamp=datetime.now(),
            version="1.0.0",
            database_connected=False,
            redis_connected=False,
            search_service_ready=False,
            uptime_seconds=0,
            metrics=Metrics(),
        )


@router.get("/metrics", response_model=Metrics)
async def get_metrics():
    """
    Obtener m√©tricas del servicio
    """
    try:
        metrics = await cache_service.get_metrics()
        if not metrics:
            return Metrics()

        return metrics

    except Exception as e:
        logger.error(f"Error obteniendo m√©tricas: {e}")
        raise HTTPException(status_code=500, detail="Error obteniendo m√©tricas")


@router.get("/cache/info")
async def get_cache_info():
    """
    Obtener informaci√≥n del cach√©
    """
    try:
        cache_info = await cache_service.get_cache_info()
        return cache_info

    except Exception as e:
        logger.error(f"Error obteniendo informaci√≥n de cach√©: {e}")
        raise HTTPException(
            status_code=500, detail="Error obteniendo informaci√≥n de cach√©"
        )


@router.delete("/cache/clear")
async def clear_cache(
    pattern: Optional[str] = Query(None, description="Patr√≥n de claves a limpiar")
):
    """
    Limpiar cach√©
    """
    try:
        if pattern:
            deleted_count = await cache_service.clear_pattern(pattern)
            return {
                "message": f"Se eliminaron {deleted_count} claves con patr√≥n '{pattern}'"
            }
        else:
            # Limpiar todo el cach√© de b√∫squeda
            deleted_count = await cache_service.clear_pattern("search")
            deleted_count += await cache_service.clear_pattern("suggestions")
            return {"message": f"Se eliminaron {deleted_count} claves del cach√©"}

    except Exception as e:
        logger.error(f"Error limpiando cach√©: {e}")
        raise HTTPException(status_code=500, detail="Error limpiando cach√©")


@router.post("/index/rebuild", response_model=BulkIndexResponse)
async def rebuild_search_index(
    request: BulkIndexRequest, background_tasks: BackgroundTasks
):
    """
    Reconstruir √≠ndice de b√∫squeda

    - **provider_ids**: Lista de IDs de proveedores a indexar (vac√≠o = todos)
    - **force_reindex**: Forzar reindexaci√≥n incluso si ya existe
    """
    try:
        # TODO: Implementar l√≥gica de reindexaci√≥n
        background_tasks.add_task(
            rebuild_index_task, request.provider_ids, request.force_reindex
        )

        return BulkIndexResponse(
            total_processed=0, successful=0, failed=0, errors=[], processing_time_ms=0
        )

    except Exception as e:
        logger.error(f"Error en reindexaci√≥n: {e}")
        raise HTTPException(status_code=500, detail="Error en reindexaci√≥n")


@router.get("/stats")
async def get_search_stats():
    """
    Obtener estad√≠sticas de b√∫squeda
    """
    try:
        # Obtener consultas populares
        popular_queries = await cache_service.get_popular_queries(10)

        # Obtener informaci√≥n de cach√©
        cache_info = await cache_service.get_cache_info()

        # TODO: Obtener estad√≠sticas de la base de datos

        return {
            "popular_queries": popular_queries,
            "cache_info": cache_info,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"Error obteniendo estad√≠sticas: {e}")
        raise HTTPException(status_code=500, detail="Error obteniendo estad√≠sticas")


# Funciones auxiliares
async def update_search_statistics(
    request_id: str, query: str, metadata: SearchMetadata
):
    """Actualizar estad√≠sticas de b√∫squeda en background"""
    try:
        # Esta funci√≥n se ejecuta en background despu√©s de responder
        await cache_service.add_query_to_popular(query)
        logger.debug(f"üìä Estad√≠sticas actualizadas para b√∫squeda [{request_id}]")
    except Exception as e:
        logger.warning(f"Error actualizando estad√≠sticas [{request_id}]: {e}")


async def rebuild_index_task(provider_ids: List[str], force_reindex: bool):
    """Tarea en background para reconstruir √≠ndice"""
    try:
        logger.info(
            f"üîÑ Iniciando reindexaci√≥n de {len(provider_ids) if provider_ids else 'todos'} proveedores"
        )
        # TODO: Implementar l√≥gica completa de reindexaci√≥n
        logger.info("‚úÖ Reindexaci√≥n completada")
    except Exception as e:
        logger.error(f"‚ùå Error en reindexaci√≥n: {e}")


# Manejador de errores global
@router.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Manejador global de excepciones"""
    logger.error(f"Error no manejado en {request.url}: {exc}")

    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            error="internal_error",
            message="Error interno del servidor",
            details={"path": str(request.url.path)},
            timestamp=datetime.now(),
            request_id=getattr(request.state, "request_id", None),
        ).model_dump(),
    )


# Middleware para logging de requests
@router.middleware("http")
async def log_requests(request, call_next):
    """Middleware para loguear todas las requests"""
    start_time = time.time()
    request_id = str(uuid.uuid4())
    request.state.request_id = request_id

    logger.info(f"üì• Request [{request_id}] {request.method} {request.url}")

    try:
        response = await call_next(request)
        process_time = int((time.time() - start_time) * 1000)

        logger.info(
            f"üì§ Response [{request_id}] {response.status_code} ({process_time}ms)"
        )

        response.headers["X-Request-ID"] = request_id
        response.headers["X-Process-Time"] = str(process_time)

        return response

    except Exception as e:
        process_time = int((time.time() - start_time) * 1000)
        logger.error(f"‚ùå Error [{request_id}] despu√©s de {process_time}ms: {e}")
        raise
