"""
Query Interpreter Service - Interpreta queries de usuarios usando IA.

Este m√≥dulo contiene la l√≥gica para interpretar mensajes de usuarios
y extraer informaci√≥n estructurada (profesi√≥n, ciudad, detalles).

DIFERENCIADOR: Usa IA para entender la intenci√≥n detr√°s de las palabras.
Ejemplo: "tengo goteras" ‚Üí "plomero"
"""

import asyncio
import json
import logging
import re
from typing import Any, Dict, Optional

from openai import AsyncOpenAI
from utils.services_utils import _safe_json_loads

# Logger del m√≥dulo
logger = logging.getLogger(__name__)


# ============================================================================
# QUERY INTERPRETER SERVICE
# ============================================================================

class QueryInterpreterService:
    """Interpreta queries de usuarios usando IA (DIFERENCIADOR).

    Responsabilidades:
    - Entender la intenci√≥n del usuario ("tengo goteras" ‚Üí "plomero")
    - Extraer profesi√≥n, ciudad y detalles
    - Fallback a interpretaci√≥n simple sin IA
    """

    def __init__(self, openai_client: AsyncOpenAI):
        """Inicializa el servicio de interpretaci√≥n de queries.

        Args:
            openai_client: Cliente OpenAI as√≠ncrono
        """
        self.client = openai_client
        self.system_prompt = """Eres un asistente experto que interpreta necesidades de servicios en Ecuador.

El usuario te dir√° lo que necesita en lenguaje natural.
Tu tarea es extraer:
1. profesion: el servicio principal (plomero, electricista, alba√±il, carpintero, pintor, marketing, etc)
2. ciudad: la ciudad donde lo necesita (si est√° expl√≠cita)
3. detalles: descripci√≥n ampliada del servicio (para enviar a proveedores)

REGLAS:
- La profesi√≥n debe ser un t√©rmino de b√∫squeda est√°ndar (ej: "plomero", no "fontanero")
- Mapeos espec√≠ficos IMPORTANTES:
  * "gestor de redes sociales" ‚Üí "marketing"
  * "community manager" ‚Üí "marketing"
  * "social media manager" ‚Üí "marketing"
  * "administrador de redes sociales" ‚Üí "marketing"
  * "redes sociales" ‚Üí "marketing"
  * "goteras" / "fugas" ‚Üí "plomero"
  * "cortocircuito" / "problemas el√©ctricos" ‚Üí "electricista"
- Si la ciudad no est√° clara, d√©jala vac√≠a
- Los detalles deben mantener el lenguaje original del usuario

Responde SOLO en JSON formato:
{
  "profesion": "plomero",
  "ciudad": "Quito",
  "detalles": "tengo goteras en el techo de la casa"
}"""

    async def interpret_query(
        self,
        user_message: str,
        city_context: Optional[str] = None,
        semaphore: Optional[asyncio.Semaphore] = None,
        timeout_seconds: float = 5.0
    ) -> Dict[str, Any]:
        """Interpreta query del usuario con IA.

        Args:
            user_message: Mensaje del usuario en lenguaje natural
            city_context: Ciudad conocida (opcional, del contexto de la conversaci√≥n)
            semaphore: Sem√°foro para limitar concurrencia OpenAI
            timeout_seconds: Timeout para llamadas a OpenAI

        Returns:
            Dict con:
                - profession: profesi√≥n extra√≠da
                - city: ciudad extra√≠da (o city_context si no se detect√≥)
                - details: detalles del servicio
        """
        # Validar que no sea un n√∫mero puro (sin contexto no tiene sentido)
        if user_message.strip().isdigit():
            logger.info(f"‚ö†Ô∏è N√∫mero puro detectado en interpret_query, rechazando: '{user_message}'")
            return {
                "profession": None,
                "city": city_context,
                "details": user_message
            }

        try:
            # Usar sem√°foro si est√° disponible
            if semaphore:
                async with semaphore:
                    response = await asyncio.wait_for(
                        self._call_openai(user_message),
                        timeout=timeout_seconds
                    )
            else:
                response = await asyncio.wait_for(
                    self._call_openai(user_message),
                    timeout=timeout_seconds
                )

            result = self._parse_openai_response(response)

            # Override ciudad si viene del contexto y IA no la detect√≥
            if city_context and not result.get("ciudad"):
                result["ciudad"] = city_context

            return {
                "profession": result.get("profesion", user_message),
                "city": result.get("ciudad", city_context or ""),
                "details": result.get("detalles", user_message)
            }

        except asyncio.TimeoutError:
            logger.warning(f"‚ö†Ô∏è Timeout interpretando query: '{user_message[:50]}...'")
            return self._fallback_interpretation(user_message, city_context)

        except Exception as e:
            logger.error(f"‚ùå Error interpretando query: {e}")
            return self._fallback_interpretation(user_message, city_context)

    async def _call_openai(self, user_message: str):
        """Llama a OpenAI para interpretar el query (m√©todo privado)."""
        return await self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": f'Interpreta esta solicitud: "{user_message}"'}
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
            max_tokens=200
        )

    def _parse_openai_response(self, response) -> Dict:
        """Parsea respuesta de OpenAI (m√©todo privado)."""
        if not response.choices:
            raise ValueError("OpenAI respondi√≥ sin choices")

        content = (response.choices[0].message.content or "").strip()

        # Limpiar markdown si est√° presente
        if content.startswith("```"):
            content = re.sub(r"^```(?:json)?", "", content, flags=re.IGNORECASE).strip()
            content = re.sub(r"```$", "", content).strip()

        parsed = _safe_json_loads(content)
        if not parsed or not isinstance(parsed, dict):
            raise ValueError(f"No se pudo parsear respuesta JSON: {content}")

        return parsed

    def _fallback_interpretation(
        self,
        message: str,
        city: Optional[str]
    ) -> Dict[str, Any]:
        """Fallback simple sin IA (m√©todo privado).

        Si IA falla, usa el mensaje tal cual como profesi√≥n.
        """
        # Si es solo un n√∫mero, no tratar como profesi√≥n
        if message.strip().isdigit():
            logger.info(f"‚ö†Ô∏è N√∫mero puro detectado en fallback, rechazando: '{message}'")
            return {
                "profession": None,
                "city": city or "",
                "details": message
            }

        logger.info(f"üîÑ Usando fallback sin IA para: '{message[:50]}...'")

        # Normalizaci√≥n simple: min√∫sculas, quitar espacios extras
        profession = message.strip().lower()

        return {
            "profession": profession,
            "city": city or "",
            "details": message
        }

    async def interpret_query_v2(
        self,
        user_message: str,
        city_context: Optional[str] = None,
        semaphore: Optional[asyncio.Semaphore] = None,
        timeout_seconds: float = 5.0,
        expand_query: bool = True
    ) -> Dict[str, Any]:
        """
        Interpreta query V2 con expansi√≥n de t√©rminos (Enhanced Search).

        MEJORAS (Plan Mejoras Inmediatas - Enero 2026):
        - Usa QueryExpander para expandir queries con sin√≥nimos
        - Cach√© en Redis para expansiones repetidas
        - Fallback a sin√≥nimos est√°ticos si OpenAI falla

        ESTRATEGIA BACKWARD COMPATIBLE:
        - Si expand_query=False, usa flujo V1 original
        - Si QueryExpander no est√° inicializado, usa flujo V1

        Args:
            user_message: Mensaje del usuario
            city_context: Ciudad conocida del contexto
            semaphore: Sem√°foro para limitar concurrencia OpenAI
            timeout_seconds: Timeout para OpenAI
            expand_query: Si True, expande query con sin√≥nimos

        Returns:
            Dict con:
                - profession: profesi√≥n interpretada
                - city: ciudad interpretada
                - details: detalles del servicio
                - expanded_terms: t√©rminos expandidos (si expand_query=True)
                - expansion_method: m√©todo usado ("cache", "ai", "static", "none")
        """
        from core.feature_flags import USE_QUERY_EXPANSION

        # Si feature flag est√° desactivado o expand_query=False, usar flujo V1
        if not USE_QUERY_EXPANSION or not expand_query:
            logger.debug("‚ö†Ô∏è USE_QUERY_EXPANSION=False o expand_query=False, usando flujo V1")
            result = await self.interpret_query(
                user_message,
                city_context,
                semaphore,
                timeout_seconds
            )
            return {
                **result,
                "expanded_terms": None,
                "expansion_method": "none"
            }

        # Flujo V2 con QueryExpander
        try:
            from services.query_expansion import get_query_expander

            expander = get_query_expander()
            if not expander:
                logger.warning("‚ö†Ô∏è QueryExpander no inicializado, usando flujo V1")
                result = await self.interpret_query(
                    user_message,
                    city_context,
                    semaphore,
                    timeout_seconds
                )
                return {
                    **result,
                    "expanded_terms": None,
                    "expansion_method": "none"
                }

            # Paso 1: Interpretar query con IA (flujo V1)
            interpreted = await self.interpret_query(
                user_message,
                city_context,
                semaphore,
                timeout_seconds
            )

            profession = interpreted.get("profession")
            city = interpreted.get("city")

            # Paso 2: Expandir query con sin√≥nimos
            expansion_result = await expander.expand_query(
                query=user_message,
                profession=profession,
                use_ai=True,
                semaphore=semaphore,
                timeout_seconds=timeout_seconds
            )

            expanded_terms = expansion_result.get("expanded_terms", [])
            inferred_profession = expansion_result.get("inferred_profession")

            logger.info(
                f"‚úÖ [V2] Query expandida: '{user_message[:30]}...' "
                f"‚Üí {len(expanded_terms)} t√©rminos "
                f"(m√©todo: {expansion_result.get('expansion_method')})"
            )

            # Si se infiri√≥ una profesi√≥n diferente, usarla
            if inferred_profession and inferred_profession != profession:
                logger.info(f"üîÆ [V2] Profesi√≥n inferida: '{profession}' ‚Üí '{inferred_profession}'")
                profession = inferred_profession

            return {
                "profession": profession or user_message,
                "city": city or city_context or "",
                "details": interpreted.get("details", user_message),
                "expanded_terms": expanded_terms,
                "expansion_method": expansion_result.get("expansion_method", "unknown"),
                "inferred_profession": inferred_profession
            }

        except Exception as e:
            logger.error(f"‚ùå [V2] Error en interpretaci√≥n con expansi√≥n: {e}")
            # Fallback a V1
            logger.info("üîÑ [V2] Fallback a flujo V1 por error")
            result = await self.interpret_query(
                user_message,
                city_context,
                semaphore,
                timeout_seconds
            )
            return {
                **result,
                "expanded_terms": None,
                "expansion_method": "fallback"
            }


# ============================================================================
# INSTANCIA GLOBAL (se inicializa en main.py)
# ============================================================================

query_interpreter: Optional[QueryInterpreterService] = None


def initialize_query_interpreter(
    openai_client: Optional[AsyncOpenAI],
    cache_manager: Optional[Any] = None
) -> None:
    """Inicializa el servicio de interpretaci√≥n de queries.

    Args:
        openai_client: Cliente OpenAI (opcional, si no hay se deshabilita)
        cache_manager: CacheManager opcional para QueryExpander
    """
    global query_interpreter

    if openai_client:
        query_interpreter = QueryInterpreterService(openai_client)
        logger.info("‚úÖ QueryInterpreterService inicializado")

        # Inicializar QueryExpander tambi√©n
        try:
            from services.query_expansion import initialize_query_expander
            initialize_query_expander(openai_client, cache_manager)
            logger.info("‚úÖ QueryExpander inicializado")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error inicializando QueryExpander: {e}")
    else:
        query_interpreter = None
        logger.warning("‚ö†Ô∏è QueryInterpreterService deshabilitado (sin OpenAI)")
