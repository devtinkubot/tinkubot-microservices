"""
Query Expansion Service para ai-clientes.

Este mÃ³dulo expande queries de bÃºsqueda con sinÃ³nimos para mejorar
la tasa de coincidencias y reducir falsos negativos.

Estrategias de expansiÃ³n:
1. CatÃ¡logo dinÃ¡mico (service_synonyms en Supabase)
2. ExpansiÃ³n con OpenAI (sinÃ³nimos regionales EC)
3. SinÃ³nimos estÃ¡ticos de backup (fallback rÃ¡pido)

CachÃ©: Redis con TTL 3600s (1 hora)
Expected cache hit rate: 70%

Author: Claude Sonnet 4.5
Created: 2026-01-15
"""

import hashlib
import json
import logging
from typing import Any, Dict, List, Optional

from openai import AsyncOpenAI

logger = logging.getLogger(__name__)


class QueryExpander:
    """
    Expande queries de bÃºsqueda con sinÃ³nimos y tÃ©rminos relacionados.

    Estrategias de expansiÃ³n (en orden de prioridad):
    1. Redis Cache (si ya se expandiÃ³ antes)
    2. CatÃ¡logo dinÃ¡mico (service_synonyms de Supabase)
    3. ExpansiÃ³n con OpenAI (IA, sinÃ³nimos regionales)
    4. SinÃ³nimos estÃ¡ticos (fallback rÃ¡pido)

    Attributes:
        openai_client: Cliente OpenAI para expansiÃ³n con IA
        cache_manager: CacheManager para Redis cache
        static_synonyms: Diccionario de sinÃ³nimos estÃ¡ticos

    Example:
        >>> expander = QueryExpander(openai_client, cache_manager)
        >>> result = await expander.expand_query("tengo goteras")
        >>> print(result["expanded_terms"])
        ['plomero', 'plomeria', 'fugas', 'agua', 'fontaneria']
    """

    # SinÃ³nimos estÃ¡ticos de backup (cuando OpenAI falla)
    STATIC_SYNONYMS: Dict[str, List[str]] = {
        "plomero": ["plomeria", "fontanero", "gasfitero", "tuberÃ­as", "fugas", "agua"],
        "electricista": ["electricidad", "elÃ©ctrico", "cableado", "instalaciÃ³n elÃ©ctrica"],
        "pintor": ["pintura", "paredes", "muros", "techumbre"],
        "estilista": ["cabello", "pelo", "corte", "tinte", "mechas", "peluquerÃ­a"],
        "esteticista": ["belleza", "facial", "cosmetologÃ­a", "skin care", "skincare"],
        "limpieza": ["limpiar", "aseo", "aseadora"],
        "jardinero": ["jardÃ­n", "cÃ©sped", "pasto", "podar", "paisajismo"],
        "veterinario": ["mascotas", "perro", "gato", "clÃ­nica veterinaria"],
        "mudanza": ["mudar", "transporte", "flete", "carga"],
        "carpintero": ["carpinterÃ­a", "madera", "muebles", "armarios", "closets"],
        "cerrajero": ["cerradura", "llave", "puerta"],
        "tÃ©cnico de computadoras": ["computadora", "pc", "laptop", "informÃ¡tica", "virus"],
        "fotÃ³grafo": ["foto", "fotografÃ­a", "retrato", "evento"],
        "mÃºsico": ["mÃºsica", "guitarra", "piano", "banda", "orquesta"],
        "cocinero": ["cocina", "chef", "comida", "restaurante"],
        "constructor": ["construcciÃ³n", "albaÃ±il", "obras", "casa"],
        "masajista": ["masaje", "masajes", "masoterapia"],
    }

    # Prompt para expansiÃ³n con OpenAI
    EXPANSION_PROMPT = """Eres un experto en servicios profesionales en Ecuador.

Expande esta consulta de bÃºsqueda incluyendo sinÃ³nimos y tÃ©rminos relacionados.

REGLAS:
- Incluye sinÃ³nimos regionales de Ecuador
- Convierte problemas a la profesiÃ³n que los resuelve
- Responde ÃšNICAMENTE con un JSON vÃ¡lido

Ejemplos:
- "tengo goteras" â†’ {{"expanded_terms": "plomero plomeria fugas agua fontaneria tuberÃ­as", "inferred_profession": "plomero"}}
- "limpieza facial" â†’ {{"expanded_terms": "estÃ©tica facial cosmetologÃ­a cuidado piel beautician spa", "inferred_profession": "esteticista"}}
- "necesito un electricista" â†’ {{"expanded_terms": "electricista electricidad instalacion cableado", "inferred_profession": "electricista"}}

Expande esta consulta: {user_message}

Responde en formato JSON:
{{"expanded_terms": "termino1 termino2 termino3 ...", "inferred_profession": "profesiÃ³n"}}"""

    def __init__(
        self,
        openai_client: AsyncOpenAI,
        cache_manager: Optional[Any] = None
    ):
        """Inicializa el expansor de queries.

        Args:
            openai_client: Cliente OpenAI asÃ­ncrono
            cache_manager: CacheManager opcional para Redis cache
        """
        self.client = openai_client
        self.cache_manager = cache_manager
        logger.debug("QueryExpander inicializado")

    async def expand_query(
        self,
        query: str,
        profession: Optional[str] = None,
        use_ai: bool = True,
        semaphore: Optional[Any] = None,
        timeout_seconds: float = 3.0
    ) -> Dict[str, Any]:
        """
        Expande una query con sinÃ³nimos y tÃ©rminos relacionados.

        Args:
            query: Query original del usuario
            profession: ProfesiÃ³n conocida (opcional, acelera expansiÃ³n)
            use_ai: Si es True, usa OpenAI para expansiÃ³n
            semaphore: SemÃ¡foro para limitar concurrencia OpenAI
            timeout_seconds: Timeout para llamadas a OpenAI

        Returns:
            Dict con:
                - expanded_terms: List[str] de tÃ©rminos expandidos
                - inferred_profession: ProfesiÃ³n inferida (si aplica)
                - expansion_method: "cache", "dynamic", "ai", o "static"

        Example:
            >>> result = await expander.expand_query("tengo goteras")
            >>> print(result)
            {
                "expanded_terms": ["plomero", "plomeria", "fugas", "agua"],
                "inferred_profession": "plomero",
                "expansion_method": "ai"
            }
        """
        # Paso 1: Verificar cachÃ© Redis
        cache_key = self._generate_cache_key(query, profession)
        if self.cache_manager:
            cached = await self._get_from_cache(cache_key)
            if cached:
                logger.debug(f"âœ… Cache HIT para query: '{query}'")
                return {
                    **cached,
                    "expansion_method": "cache"
                }

        # Paso 2: Intentar expansiÃ³n con catÃ¡logo dinÃ¡mico
        dynamic_result = await self._expand_with_dynamic_catalog(query, profession)
        if dynamic_result:
            logger.debug(f"âœ… ExpansiÃ³n con catÃ¡logo dinÃ¡mico para: '{query}'")
            # Guardar en cachÃ©
            await self._save_to_cache(cache_key, dynamic_result)
            return {
                **dynamic_result,
                "expansion_method": "dynamic"
            }

        # Paso 3: ExpansiÃ³n con OpenAI (si estÃ¡ habilitado)
        if use_ai:
            try:
                ai_result = await self._expand_with_openai(
                    query,
                    semaphore=semaphore,
                    timeout_seconds=timeout_seconds
                )
                if ai_result:
                    logger.debug(f"âœ… ExpansiÃ³n con OpenAI para: '{query}'")
                    # Guardar en cachÃ©
                    await self._save_to_cache(cache_key, ai_result)
                    return {
                        **ai_result,
                        "expansion_method": "ai"
                    }
            except Exception as e:
                logger.warning(f"âš ï¸ Error en expansiÃ³n OpenAI: {e}, usando fallback")

        # Paso 4: Fallback a sinÃ³nimos estÃ¡ticos
        static_result = self._expand_with_static_synonyms(query, profession)
        logger.debug(f"âœ… ExpansiÃ³n con sinÃ³nimos estÃ¡ticos para: '{query}'")
        # Guardar en cachÃ©
        await self._save_to_cache(cache_key, static_result)
        return {
            **static_result,
            "expansion_method": "static"
        }

    def _generate_cache_key(self, query: str, profession: Optional[str]) -> str:
        """Genera clave de cachÃ© para la query."""
        key_data = f"{query}:{profession or ''}"
        return f"query_expansion:{hashlib.md5(key_data.encode()).hexdigest()}"

    async def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Obtiene expansiÃ³n desde cachÃ© Redis."""
        if not self.cache_manager:
            return None

        try:
            from core.cache import CacheNamespace

            cached = await self.cache_manager.get(
                CacheNamespace.SEARCH_RESULTS,
                cache_key
            )

            if cached:
                return json.loads(cached) if isinstance(cached, str) else cached
        except Exception as e:
            logger.warning(f"âš ï¸ Error leyendo cachÃ©: {e}")

        return None

    async def _save_to_cache(self, cache_key: str, data: Dict[str, Any]) -> None:
        """Guarda expansiÃ³n en cachÃ© Redis."""
        if not self.cache_manager:
            return

        try:
            from core.cache import CacheNamespace

            await self.cache_manager.set(
                CacheNamespace.SEARCH_RESULTS,
                cache_key,
                data,
                ttl=3600  # 1 hora
            )
            logger.debug(f"ðŸ’¾ Guardado en cachÃ©: {cache_key}")
        except Exception as e:
            logger.warning(f"âš ï¸ Error guardando en cachÃ©: {e}")

    async def _expand_with_dynamic_catalog(
        self,
        query: str,
        profession: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """Expande query usando catÃ¡logo dinÃ¡mico de sinÃ³nimos."""
        try:
            from services.dynamic_service_catalog import dynamic_service_catalog

            # Buscar sinÃ³nimos para la profesiÃ³n
            canonical = await dynamic_service_catalog.find_profession(query)

            if canonical:
                # Obtener sinÃ³nimos del catÃ¡logo (retorna Dict[str, Set[str]])
                all_synonyms = await dynamic_service_catalog.get_synonyms()

                if all_synonyms and canonical in all_synonyms:
                    synonym_set = all_synonyms[canonical]
                    # Convertir Set[str] a List[str]
                    synonyms_list = list(synonym_set)

                    if synonyms_list:
                        return {
                            "expanded_terms": [canonical] + synonyms_list,
                            "inferred_profession": canonical
                        }

            return None
        except Exception as e:
            logger.debug(f"CatÃ¡logo dinÃ¡mico no disponible: {e}")
            return None

    async def _expand_with_openai(
        self,
        query: str,
        semaphore: Optional[Any] = None,
        timeout_seconds: float = 3.0
    ) -> Optional[Dict[str, Any]]:
        """Expande query usando OpenAI."""
        try:
            # Aplicar semÃ¡foro si existe
            if semaphore:
                await semaphore.acquire()

            try:
                response = await self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "Eres un experto en servicios en Ecuador."},
                        {"role": "user", "content": self.EXPANSION_PROMPT.format(user_message=query)}
                    ],
                    temperature=0.3,
                    max_tokens=100,
                    timeout=timeout_seconds
                )

                content = response.choices[0].message.content.strip()

                # Parsear JSON de respuesta
                result = json.loads(content)

                # Convertir expanded_terms de string a lista
                if isinstance(result.get("expanded_terms"), str):
                    result["expanded_terms"] = result["expanded_terms"].split()

                return result

            finally:
                if semaphore:
                    semaphore.release()

        except Exception as e:
            logger.warning(f"âš ï¸ Error en expansiÃ³n OpenAI: {e}")
            return None

    def _expand_with_static_synonyms(
        self,
        query: str,
        profession: Optional[str]
    ) -> Dict[str, Any]:
        """Expande query usando sinÃ³nimos estÃ¡ticos (fallback)."""
        query_lower = query.lower()

        # Buscar profesiÃ³n en sinÃ³nimos estÃ¡ticos
        for canonical_profession, synonyms in self.STATIC_SYNONYMS.items():
            # Verificar si la query menciona esta profesiÃ³n o sinÃ³nimos
            if canonical_profession in query_lower:
                return {
                    "expanded_terms": [canonical_profession] + synonyms,
                    "inferred_profession": canonical_profession
                }

            # Verificar sinÃ³nimos
            for synonym in synonyms:
                if synonym in query_lower:
                    return {
                        "expanded_terms": [canonical_profession] + synonyms,
                        "inferred_profession": canonical_profession
                    }

        # Si no se encontrÃ³, retornar la query original tokenizada
        tokens = query_lower.split()
        return {
            "expanded_terms": tokens,
            "inferred_profession": profession or (tokens[0] if tokens else None)
        }


# Instancia global (singleton)
_query_expander: Optional[QueryExpander] = None


def get_query_expander() -> Optional[QueryExpander]:
    """
    Retorna la instancia global del QueryExpander (singleton).

    Returns:
        QueryExpander: Instancia del expansor

    Example:
        >>> from services.query_expansion import get_query_expander
        >>> expander = get_query_expander()
        >>> result = await expander.expand_query("tengo goteras")
    """
    global _query_expander
    return _query_expander


def initialize_query_expander(
    openai_client: AsyncOpenAI,
    cache_manager: Optional[Any] = None
) -> QueryExpander:
    """
    Inicializa el QueryExpander global.

    Args:
        openai_client: Cliente OpenAI
        cache_manager: CacheManager opcional

    Returns:
        QueryExpander: Instancia inicializada
    """
    global _query_expander
    _query_expander = QueryExpander(openai_client, cache_manager)
    logger.info("âœ… QueryExpander singleton inicializado")
    return _query_expander
