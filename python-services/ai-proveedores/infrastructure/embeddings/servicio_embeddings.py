"""
Servicio de generaciÃ³n de embeddings con OpenAI.

Este servicio proporciona funcionalidad para generar embeddings de texto
utilizando el modelo text-embedding-3-small de OpenAI. Incluye cachÃ© en Redis
para evitar llamadas duplicadas y reducir costos.

CaracterÃ­sticas principales:
- GeneraciÃ³n de embeddings individuales o por lotes
- CachÃ© en Redis con TTL configurable
- Manejo de errores con reintentos
- Timeout configurable para llamadas a OpenAI
- Logging completo de operaciones
"""

import hashlib
import json
import logging
from typing import List, Optional

from openai import AsyncOpenAI

from infrastructure.redis.cliente_redis import get_redis_client

logger = logging.getLogger(__name__)


class ServicioEmbeddings:
    """
    Servicio para generar embeddings con OpenAI.

    Este servicio encapsula la lÃ³gica de generaciÃ³n de embeddings utilizando
    la API de OpenAI, con optimizaciones como cachÃ© en Redis para reducir
    costos y mejorar performance.

    Atributos:
        client: Cliente asÃ­ncrono de OpenAI
        model: Modelo de embeddings a usar (default: text-embedding-3-small)
        cache_ttl: Tiempo de vida del cachÃ© en segundos (default: 3600)
        timeout: Timeout en segundos para llamadas a OpenAI (default: 5)

    Ejemplo:
        >>> from openai import AsyncOpenAI
        >>> cliente = AsyncOpenAI(api_key="...")
        >>> servicio = ServicioEmbeddings(cliente)
        >>> embedding = await servicio.generar_embedding("plomerÃ­a")
        >>> print(len(embedding))  # 1536
    """

    def __init__(
        self,
        cliente_openai: AsyncOpenAI,
        modelo: str = "text-embedding-3-small",
        cache_ttl: int = 3600,
        timeout: int = 5,
    ):
        """
        Inicializa el servicio de embeddings.

        Args:
            cliente_openai: Cliente asÃ­ncrono de OpenAI
            modelo: Modelo de embeddings a usar (default: text-embedding-3-small)
            cache_ttl: Tiempo de vida del cachÃ© en segundos (default: 3600 = 1 hora)
            timeout: Timeout en segundos para llamadas a OpenAI (default: 5)
        """
        self.client = cliente_openai
        self.model = modelo
        self.cache_ttl = cache_ttl
        self.timeout = timeout
        self._redis = None

    async def _obtener_redis(self):
        """
        Obtiene el cliente de Redis de forma lazy.

        Returns:
            Cliente de Redis o None si no estÃ¡ disponible
        """
        if self._redis is None:
            try:
                self._redis = await get_redis_client()
            except Exception as e:
                logger.warning(f"âš ï¸ No se pudo conectar a Redis: {e}")
        return self._redis

    def _generar_clave_cache(self, texto: str) -> str:
        """
        Genera una clave de cachÃ© Ãºnica para el texto.

        Usa SHA256 para generar un hash corto y Ãºnico del texto,
        lo que permite usar como clave en Redis sin problemas de longitud.

        Args:
            texto: Texto a cachear

        Returns:
            Clave de cachÃ© con formato: embedding:{modelo}:{hash}
        """
        # Generar hash del texto para clave corta
        texto_hash = hashlib.sha256(texto.encode()).hexdigest()[:16]
        return f"embedding:{self.model}:{texto_hash}"

    async def generar_embedding(
        self,
        texto: str,
        usar_cache: bool = True,
    ) -> Optional[List[float]]:
        """
        Genera embedding para un texto individual.

        Este mÃ©todo genera un embedding vectorial de 1536 dimensiones
        utilizando el modelo configurado de OpenAI. Si el cachÃ© estÃ¡
        habilitado y el texto ya fue procesado, retorna el resultado
        cacheado para reducir costos.

        Args:
            texto: Texto a convertir en embedding
            usar_cache: Si True, intenta usar cachÃ© de Redis primero

        Returns:
            Lista de 1536 floats representando el embedding, o None si fallÃ³

        Raises:
            ValueError: Si el texto estÃ¡ vacÃ­o o es solo espacios

        Ejemplo:
            >>> embedding = await servicio.generar_embedding("PlomerÃ­a y FontanerÃ­a")
            >>> print(len(embedding))  # 1536
        """
        if not texto or not texto.strip():
            raise ValueError("El texto no puede estar vacÃ­o")

        # Limpiar texto
        texto = texto.strip()

        # Verificar cachÃ© si estÃ¡ habilitado
        if usar_cache:
            try:
                redis = await self._obtener_redis()
                if redis:
                    clave_cache = self._generar_clave_cache(texto)
                    cachado = await redis.get(clave_cache)
                    if cachado:
                        logger.debug(f"âœ… Cache hit para: {texto[:50]}...")
                        return json.loads(cachado)
            except Exception as e:
                logger.warning(f"âš ï¸ Error leyendo cachÃ©: {e}")

        # Generar embedding con OpenAI
        try:
            logger.info(f"ğŸ”„ Generando embedding para: {texto[:50]}...")
            respuesta = await self.client.embeddings.create(
                input=texto,
                model=self.model,
                timeout=self.timeout,
            )

            embedding = respuesta.data[0].embedding

            # Guardar en cachÃ©
            if usar_cache:
                try:
                    redis = await self._obtener_redis()
                    if redis:
                        clave_cache = self._generar_clave_cache(texto)
                        await redis.set(
                            clave_cache,
                            json.dumps(embedding),
                            ex=self.cache_ttl,
                        )
                        logger.debug(f"ğŸ’¾ Embedding cacheado: {clave_cache}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Error guardando cachÃ©: {e}")

            logger.info(f"âœ… Embedding generado: {len(embedding)} dimensiones")
            return embedding

        except Exception as e:
            logger.error(f"âŒ Error generando embedding: {e}")
            return None

    async def generar_embedding_lote(
        self,
        textos: List[str],
        usar_cache: bool = True,
    ) -> List[Optional[List[float]]]:
        """
        Genera embeddings para mÃºltiples textos en lote.

        Este mÃ©todo procesa una lista de textos y genera un embedding
        para cada uno. Es Ãºtil para backfill o procesamiento por lotes.

        Args:
            textos: Lista de textos a convertir
            usar_cache: Si True, usa cachÃ© para textos ya procesados

        Returns:
            Lista de embeddings (misma longitud que textos de entrada)
            Cada elemento es List[float] o None si fallÃ³ para ese texto

        Ejemplo:
            >>> servicios = ["PlomerÃ­a", "Electricidad", "GasfiterÃ­a"]
            >>> embeddings = await servicio.generar_embedding_lote(servicios)
            >>> print(len(embeddings))  # 3
        """
        if not textos:
            return []

        embeddings = []

        for texto in textos:
            embedding = await self.generar_embedding(texto, usar_cache=usar_cache)
            embeddings.append(embedding)

        logger.info(f"âœ… Lote completado: {len(embeddings)} embeddings generados")
        return embeddings

    async def generar_embedding_servicios(
        self,
        servicios: List[str],
    ) -> List[Optional[List[float]]]:
        """
        Genera embeddings individuales para una lista de servicios.

        A diferencia de generar_embedding_lote() que genera un embedding
        por texto, este mÃ©todo estÃ¡ optimizado especÃ­ficamente para servicios
        de proveedores, generando un embedding individual por servicio.

        Args:
            servicios: Lista de servicios del proveedor (ej: ["PlomerÃ­a", "Electricidad"])

        Returns:
            Lista de embeddings, uno por servicio

        Ejemplo:
            >>> servicios = ["PlomerÃ­a", "Electricidad", "GasfiterÃ­a"]
            >>> embeddings = await servicio.generar_embedding_servicios(servicios)
            >>> print(len(embeddings))  # 3
            >>> print(len(embeddings[0]))  # 1536
        """
        if not servicios:
            logger.warning("âš ï¸ Lista de servicios vacÃ­a, no se generan embeddings")
            return []

        logger.info(f"ğŸ”„ Generando embeddings para {len(servicios)} servicios...")

        # Generar un embedding por servicio individualmente
        embeddings = await self.generar_embedding_lote(servicios, usar_cache=True)

        embeddings_validos = [e for e in embeddings if e is not None]
        logger.info(f"âœ… {len(embeddings_validos)}/{len(servicios)} embeddings generados exitosamente")

        return embeddings

    async def verificar_embedding_en_cache(self, texto: str) -> bool:
        """
        Verifica si un embedding ya existe en cachÃ©.

        Args:
            texto: Texto a verificar

        Returns:
            True si existe en cachÃ©, False en caso contrario
        """
        try:
            redis = await self._obtener_redis()
            if redis:
                clave_cache = self._generar_clave_cache(texto)
                return await redis.exists(clave_cache) > 0
        except Exception as e:
            logger.warning(f"âš ï¸ Error verificando cachÃ©: {e}")

        return False

    async def limpiar_cache(self, texto: Optional[str] = None) -> int:
        """
        Limpia embeddings de la cachÃ©.

        Args:
            texto: Si se proporciona, limpia solo ese texto.
                   Si es None, limpia todos los embeddings del modelo.

        Returns:
            NÃºmero de claves eliminadas

        Ejemplo:
            >>> # Limpiar un texto especÃ­fico
            >>> await servicio.limpiar_cache("PlomerÃ­a")
            >>> # Limpiar todos los embeddings
            >>> await servicio.limpiar_cache()
        """
        try:
            redis = await self._obtener_redis()
            if not redis:
                logger.warning("âš ï¸ Redis no disponible para limpiar cachÃ©")
                return 0

            if texto:
                clave = self._generar_clave_cache(texto)
                eliminados = await redis.delete(clave)
                logger.info(f"ğŸ—‘ï¸ Eliminado {eliminados} embedding de cachÃ©")
            else:
                # Eliminar todos los embeddings del modelo actual
                patron = f"embedding:{self.model}:*"
                claves = await redis.keys(patron)
                if claves:
                    eliminados = await redis.delete(*claves)
                    logger.info(f"ğŸ—‘ï¸ Eliminados {eliminados} embeddings de cachÃ©")
                else:
                    eliminados = 0

            return eliminados

        except Exception as e:
            logger.error(f"âŒ Error limpiando cachÃ©: {e}")
            return 0
