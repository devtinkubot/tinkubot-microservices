# Plan: B√∫squeda Sem√°ntica de Proveedores con Hugging Face

> **√öltima actualizaci√≥n**: Enero 2025
> **Versi√≥n**: 2.0 - Actualizada con nueva arquitectura SOLID
> **Estado**: Listo para implementaci√≥n

## Resumen Ejecutivo

Implementar b√∫squeda sem√°ntica para mejorar la interpretaci√≥n del lenguaje natural de clientes y encontrar mejores matches con proveedores, usando **sentence-transformers** pre-entrenados v√≠a **Hugging Face Inference API**.

**Enfoque**: Extensi√≥n de arquitectura existente - 1-2 semanas de implementaci√≥n
**Prioridad**: Mejor interpretaci√≥n del lenguaje del cliente ("tengo goteras" ‚Üí "plomero")
**Infraestructura**: Hugging Face Inference API + cach√© Redis + pgvector
**Arquitectura**: Aprovecha patrones SOLID ya implementados (Repository, Cache, Metrics, Feature Flags)

---

## Arquitectura Actual Implementada (Enero 2025)

### Patrones Arquitect√≥nicos Activos en ai-clientes

| Patr√≥n | Estado | Archivos | Feature Flag | Descripci√≥n |
|--------|--------|----------|--------------|-------------|
| **Repository Pattern** | ‚úÖ Activo | `repositories/provider_repository.py` | `USE_REPOSITORY_INTERFACES = True` | Abstracci√≥n de acceso a datos |
| **State Machine** | ‚úÖ Activo | `core/state_machine.py` | `USE_STATE_MACHINE = True` | Validaci√≥n de transiciones de conversaci√≥n |
| **Saga Pattern** | ‚úÖ Activo | `core/saga.py` | `USE_SAGA_ROLLBACK = True` | Rollback transaccional autom√°tico |
| **Cache Layer** | ‚úÖ Activo | `core/cache.py` | `ENABLE_PERFORMANCE_OPTIMIZATIONS = True` | Cach√© Redis con namespaces y TTLs |
| **Performance Metrics** | ‚úÖ Activo | `core/metrics.py` | `ENABLE_PERFORMANCE_OPTIMIZATIONS = True` | Tracking de latencias (p50, p95, p99) |
| **Feature Flags** | ‚úÖ Activo | `core/feature_flags.py` | `ENABLE_FEATURE_FLAGS = True` | Sistema de migraci√≥n gradual (5 fases) |

### Servicios Core Existentes

| Servicio | Ubicaci√≥n | Responsabilidad |
|----------|-----------|-----------------|
| **QueryInterpreterService** | `ai-clientes/services/query_interpreter_service.py` | Interpreta lenguaje natural con OpenAI GPT-3.5 |
| **SearchService** | `ai-clientes/services/search_service.py` | B√∫squeda de proveedores (directo a Supabase via Repository) |
| **ProviderRepository** | `ai-proveedores/repositories/provider_repository.py` | Acceso a datos de proveedores con interface `IProviderRepository` |
| **CacheManager** | `ai-clientes/core/cache.py` | Cach√© Redis con namespaces (SEARCH_RESULTS, CUSTOMER_PROFILE, etc.) |
| **PerformanceMetrics** | `ai-clientes/core/metrics.py` | M√©tricas de performance (min, max, avg, p50, p95, p99) |

### Servicios Eliminados (Historia)

- ‚ùå **ai-search**: Eliminado en Sprint 2.4 (SPOF eliminado)
- B√∫squeda ahora es directa a Supabase v√≠a ProviderRepository
- Esto simplifica la arquitectura y mejora la mantenibilidad

---

## Problema Actual

### B√∫squeda Actual (ai-clientes/services/search_service.py)

```python
# B√∫squeda actual usa Repository Pattern
async def intelligent_search_providers(payload: Dict[str, Any]) -> Dict[str, Any]:
    # 1. IA interpreta la query
    interpretation = await query_interpreter_svc.interpret_query(
        user_message=query,
        city_context=location,
        semaphore=openai_semaphore,
        timeout_seconds=OPENAI_TIMEOUT_SECONDS
    )

    # 2. B√∫squeda en Supabase v√≠a ProviderRepository (DIRECTO, sin SPOF)
    providers = await provider_repo.search_by_city_and_profession(
        city=interpreted_city,
        profession=interpreted_profession,
        limit=10
    )
```

**Limitaciones**:
- ‚ö†Ô∏è Interpreta bien con OpenAI (~70% precisi√≥n)
- ‚ùå B√∫squeda por texto exacto (ILIKE): `WHERE profession ILIKE '%plomero%'`
- ‚ùå No encuentra proveedores sem√°nticamente similares
- ‚ùå Falsos negativos: "tengo goteras" NO encuentra plomeros si no dice "plomero"

---

## Soluci√≥n Propuesta

### Arquitectura Actual (Sin Embeddings)

```
Cliente: "tengo goteras"
   ‚Üì
AI Clientes: QueryInterpreterService (OpenAI GPT-3.5)
   ‚Üí Interpreta: "plomero" + ciudad
   ‚Üì
AI Clientes: ProviderRepository (interface)
   ‚Üí Supabase: providers table
   ‚Üí B√∫squeda: WHERE profession ILIKE '%plomero%' AND city = 'Quito'
   ‚Üì
CacheManager (Redis)
   ‚Üí Cache TTL: 300s (5 min) para b√∫squedas
   ‚Üì
PerformanceMetrics
   ‚Üí Tracking: p50, p95, p99 latencias
   ‚Üì
Retorna: Lista de proveedores
```

### Arquitectura Propuesta (Con Embeddings - Fase 6)

```
Cliente: "tengo goteras"
   ‚Üì
AI Clientes: QueryInterpreterService (OpenAI GPT-3.5)
   ‚Üí Interpreta: "plomero" + ciudad + detalles
   ‚Üì
[NEW] EmbeddingService (ai-clientes/services/embedding_service.py)
   ‚Üí HF Inference API
   ‚Üí Genera embedding: [0.23, -0.45, ...] (384 dims)
   ‚Üì
[NEW] CacheManager (Embeddings)
   ‚Üí Redis cache: key="embedding:hash(query)"
   ‚Üí TTL: 3600s (1 hora)
   ‚Üì
[NEW] PostgreSQL + pgvector
   ‚Üí SELECT * FROM match_providers_semantic(
       query_embedding,
       target_city := 'Quito',
       max_results := 10
     )
   ‚Üí ORDER BY cosine_similarity DESC
   ‚Üì
PerformanceMetrics
   ‚Üí M√©tricas adicionales:
      - embedding_generation_ms
      - semantic_search_ms
      - cache_hit_rate_embeddings
   ‚Üì
Retorna: Lista de proveedores ordenados por similitud (0-1)
```

### Tecnolog√≠as

**Modelo**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- 384 dimensiones (ligero, r√°pido)
- Multiling√ºe (espa√±ol incluido)
- Gratis en HF Inference API (hasta ~1000 queries/d√≠a)

**Infraestructura**:
- **Inferencia**: HF Inference API + fallback local (sentence-transformers)
- **Base de datos**: PostgreSQL con extensi√≥n **pgvector**
- **Cach√©**: Redis (ya existe - CacheManager)
- **M√©tricas**: PerformanceMetrics (ya existe)
- **Feature Flags**: Sistema de flags (ya existe)

---

## Fases de Implementaci√≥n Actualizadas

### Fase 0: Verificaci√≥n de Arquitectura (D√≠a 0.5)

**Verificar que la arquitectura actual soporta la extensi√≥n:**

1. ‚úÖ Confirmar que ProviderRepository existe
2. ‚úÖ Confirmar que QueryInterpreterService existe
3. ‚úÖ Confirmar que CacheManager existe (para cachear embeddings)
4. ‚úÖ Confirmar que PerformanceMetrics existe (para tracking)
5. ‚ùå Verificar que pgvector est√° disponible en Supabase
6. ‚ùå Crear tabla `provider_embeddings` con √≠ndice HNSW

**Comando de verificaci√≥n:**
```bash
# Verificar extensi√≥n pgvector en Supabase
psql $DATABASE_URL -c "SELECT extname, extversion FROM pg_extension WHERE extname = 'vector';"

# Esperado: extensi√≥n 'vector' instalada
# Si no est√°: crear ticket con Soporte Supabase
```

---

### Fase 1: Setup de Base de Datos (D√≠a 1-2)

**Archivo**: `python-services/ai-proveedores/migrations/add_semantic_search.sql` (NUEVO)

**IMPORTANTE**: Esta migraci√≥n es **ADDITIVA** - NO rompe nada existente.

```sql
-- ============================================================================
-- Migraci√≥n: B√∫squeda Sem√°ntica de Proveedores
-- Fecha: Enero 2025
-- Descripci√≥n: Agrega embeddings y pgvector para b√∫squeda sem√°ntica
-- ============================================================================

-- 1. Crear tabla de embeddings (NUEVA, INDEPENDIENTE)
CREATE TABLE IF NOT EXISTS provider_embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    provider_id VARCHAR(255) UNIQUE NOT NULL,

    -- Embedding vector (384 dimensiones para MiniLM-L12-v2)
    full_profile_embedding vector(384) NOT NULL,

    -- Metadata
    embedding_model VARCHAR(100) DEFAULT 'paraphrase-multilingual-MiniLM-L12-v2',
    embedding_version VARCHAR(10) DEFAULT '1.0',

    -- Timestamps
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),

    -- Foreign key (con CASCADE delete si se borra provider)
    CONSTRAINT fk_provider_embeddings
        FOREIGN KEY (provider_id)
        REFERENCES providers(id)
        ON DELETE CASCADE
);

-- 2. Crear √≠ndice HNSW para b√∫squeda r√°pida (Hierarchical Navigable Small World)
CREATE INDEX IF NOT EXISTS idx_embeddings_hnsw
ON provider_embeddings
USING hnsw (full_profile_embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- 3. Crear √≠ndice GIN para b√∫squedas por metadata (opcional)
CREATE INDEX IF NOT EXISTS idx_embeddings_provider_id
ON provider_embeddings (provider_id);

-- 4. Trigger para updated_at autom√°tico
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_provider_embeddings_updated_at
    BEFORE UPDATE ON provider_embeddings
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- 5. Funci√≥n de b√∫squeda sem√°ntica (mejorada con filtros)
CREATE OR REPLACE FUNCTION match_providers_semantic(
    query_embedding vector(384),
    target_city VARCHAR DEFAULT NULL,
    max_results INT DEFAULT 10,
    min_similarity FLOAT DEFAULT 0.5
) RETURNS TABLE (
    provider_id VARCHAR,
    phone VARCHAR,
    name VARCHAR,
    profession VARCHAR,
    city VARCHAR,
    services TEXT,
    specialty TEXT,
    rating DECIMAL,
    verified BOOLEAN,
    available BOOLEAN,
    similarity FLOAT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        p.id,
        p.phone,
        p.name,
        p.profession,
        p.city,
        p.services,
        p.specialty,
        p.rating,
        p.verified,
        p.available,
        -- Cosine similarity: 1 - cosine_distance
        (1 - (pe.full_profile_embedding <=> query_embedding)) as similarity
    FROM provider_embeddings pe
    INNER JOIN providers p ON p.id = pe.provider_id
    WHERE
        p.verified = true
        AND p.available = true
        AND (target_city IS NULL OR p.city ILIKE '%' || target_city || '%')
        AND (1 - (pe.full_profile_embedding <=> query_embedding)) >= min_similarity
    ORDER BY pe.full_profile_embedding <=> query_embedding
    LIMIT max_results;
END;
$$ LANGUAGE plpgsql STABLE;

-- 6. Comentario para documentaci√≥n
COMMENT ON FUNCTION match_providers_semantic IS
'B√∫squeda sem√°ntica de proveedores usando cosine similarity en embeddings de 384 dimensiones.

Args:
  - query_embedding: Embedding del query del usuario (384 dimensiones, vector)
  - target_city: Filtro opcional por ciudad (b√∫squeda ILIKE parcial)
  - max_results: M√°ximo n√∫mero de resultados a retornar (default: 10)
  - min_similarity: Similitud m√≠nima para incluir resultados (0-1, default: 0.5)

Returns:
  - Proveedores ordenados por similitud descendente (similarity de 0 a 1)
  - Incluye metadata del proveedor y score de similitud

Example:
  SELECT * FROM match_providers_semantic(
    '[0.1, 0.2, ...]'::vector(384),
    'Quito',
    10,
    0.7
  );
';

-- 7. Crear tabla para recolectar datos de entrenamiento (futuro)
CREATE TABLE IF NOT EXISTS search_interactions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id VARCHAR(255),
    client_phone VARCHAR(50),

    -- Query original e interpretaci√≥n
    original_query TEXT NOT NULL,
    interpreted_profession VARCHAR(255),
    interpreted_city VARCHAR(255),
    query_details TEXT,

    -- Resultados
    providers_shown JSONB,
    providers_count INT,
    provider_contacted VARCHAR(255),

    -- Feedback
    successful_match BOOLEAN,
    contact_made BOOLEAN,

    -- Metadata
    search_method VARCHAR(50) DEFAULT 'semantic',
    similarity_scores JSONB,

    timestamp TIMESTAMP DEFAULT NOW()
);

-- √çndices para analytics
CREATE INDEX IF NOT EXISTS idx_search_interactions_phone
ON search_interactions (client_phone);

CREATE INDEX IF NOT EXISTS idx_search_interactions_timestamp
ON search_interactions (timestamp DESC);

-- 8. Grant permissions (si es necesario)
-- GRANT ALL ON TABLE provider_embeddings TO service_role;
-- GRANT EXECUTE ON FUNCTION match_providers_semantic TO service_role;
```

**Validaci√≥n de migraci√≥n:**
```bash
# Ejecutar migraci√≥n
psql $DATABASE_URL -f migrations/add_semantic_search.sql

# Verificar tabla creada
psql $DATABASE_URL -c "\d provider_embeddings"

# Verificar √≠ndice HNSW creado
psql $DATABASE_URL -c "\di idx_embeddings_hnsw"

# Verificar funci√≥n creada
psql $DATABASE_URL -c "\df match_providers_semantic"
```

---

### Fase 2: Generar Embeddings (D√≠a 3-4)

**Archivo**: `python-services/ai-proveedores/scripts/generate_embeddings.py` (NUEVO)

**Caracter√≠sticas mejoradas:**
- ‚úÖ Usa ProviderRepository existente (no crea nueva conexi√≥n)
- ‚úÖ Integra con PerformanceMetrics para tracking
- ‚úÖ Soporta HF Inference API o modelo local
- ‚úÖ Manejo robusto de errores con reintentos
- ‚úÖ Modo test para debugging (5 proveedores)

```python
#!/usr/bin/env python3
"""
Genera embeddings para proveedores existentes usando HF Inference API.

Integraci√≥n con arquitectura SOLID existente:
- ProviderRepository: Acceso a datos de proveedores
- PerformanceMetrics: Tracking de latencias
- CacheManager: Cach√© de embeddings generados

Uso:
    python scripts/generate_embeddings.py [--local] [--test]

Args:
    --local: Usa modelo local (sentence-transformers) en lugar de HF API
    --test: Modo prueba (solo 5 proveedores)
    --batch-size: Tama√±o de batch para HF API (default: 10)

Dependencias:
    pip install sentence-transformers numpy httpx
"""

import asyncio
import argparse
import logging
import os
from typing import List, Dict, Any
import sys

# Agregar parent directory al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from repositories.provider_repository import SupabaseProviderRepository
from core.metrics import metrics
from app.dependencies import get_supabase_client

logger = logging.getLogger(__name__)


class EmbeddingGenerator:
    """Generador de embeddings para proveedores."""

    MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
    EMBEDDING_DIMS = 384

    def __init__(self, use_local: bool = False, batch_size: int = 10):
        """
        Inicializa el generador.

        Args:
            use_local: Si True, usa modelo local (m√°s lento, sin costos externos)
            batch_size: Tama√±o de batch para HF API
        """
        self.use_local = use_local
        self.batch_size = batch_size
        self._local_model = None

        if use_local:
            logger.info("üîÑ Cargando modelo local (puede tomar unos segundos)...")
            from sentence_transformers import SentenceTransformer
            self._local_model = SentenceTransformer(self.MODEL_NAME)
            logger.info("‚úÖ Modelo local cargado")
        else:
            logger.info("‚úÖ Usando HF Inference API")

    def _prepare_provider_text(self, provider: Dict[str, Any]) -> str:
        """
        Prepara el texto del proveedor para generar embedding.

        Args:
            provider: Datos del proveedor

        Returns:
            Texto formateado para embedding
        """
        parts = [
            f"Profesi√≥n: {provider.get('profession', '')}",
            f"Servicios: {provider.get('services', '')}",
            f"Especialidad: {provider.get('specialty', '')}",
            f"Ciudad: {provider.get('city', '')}",
            f"Descripci√≥n: {provider.get('description', '')}",
        ]

        # Filtrar partes vac√≠as
        text = ". ".join([p for p in parts if ':' in p and p.split(': ')[1]])
        return text

    async def _generate_embedding_hf(self, text: str) -> List[float]:
        """
        Genera embedding usando HF Inference API.

        Args:
            text: Texto a embeddar

        Returns:
            Lista de floats (384 dimensiones)
        """
        import httpx
        import numpy as np

        hf_token = os.getenv('HF_TOKEN')
        api_url = f"https://api-inference.huggingface.co/models/{self.MODEL_NAME}"

        if not hf_token:
            raise Exception("HF_TOKEN no est√° configurado")

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                api_url,
                headers={"Authorization": f"Bearer {hf_token}"},
                json={"inputs": text}
            )

            if response.status_code == 200:
                embedding = np.array(response.json()[0])
                # Normalizar embedding (norm L2)
                embedding = embedding / np.linalg.norm(embedding)
                return embedding.tolist()
            else:
                error_text = response.text
                raise Exception(f"HF API error {response.status_code}: {error_text}")

    async def _generate_embedding_local(self, text: str) -> List[float]:
        """
        Genera embedding usando modelo local (fallback).

        Args:
            text: Texto a embeddar

        Returns:
            Lista de floats (384 dimensiones)
        """
        if self._local_model is None:
            raise Exception("Modelo local no cargado")

        # Generar embedding
        embedding = self._local_model.encode(
            text,
            normalize_embeddings=True,
            show_progress_bar=False
        )

        return embedding.tolist()

    async def generate_for_provider(
        self,
        provider: Dict[str, Any],
        supabase
    ) -> bool:
        """
        Genera y guarda embedding para un proveedor.

        Args:
            provider: Datos del proveedor
            supabase: Cliente Supabase

        Returns:
            True si exitoso, False en caso contrario
        """
        try:
            provider_id = provider.get('id')
            if not provider_id:
                logger.warning(f"‚ö†Ô∏è Provider sin ID, saltando")
                return False

            # Preparar texto
            text = self._prepare_provider_text(provider)

            # Generar embedding
            async with metrics.timer("embedding_generation"):
                if self.use_local:
                    embedding = await self._generate_embedding_local(text)
                else:
                    embedding = await self._generate_embedding_hf(text)

            # Guardar en BD (upsert para no duplicar)
            # Convertir lista a string de vectores para pgvector
            embedding_str = str(embedding)

            supabase.table('provider_embeddings').upsert({
                'provider_id': provider_id,
                'full_profile_embedding': embedding_str,
                'embedding_model': self.MODEL_NAME,
            }, on_conflict='provider_id').execute()

            logger.info(f"‚úÖ Embedding generado para provider {provider_id}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Error generando embedding para provider {provider.get('id')}: {e}")
            return False

    async def generate_batch(
        self,
        providers: List[Dict[str, Any]],
        supabase
    ) -> Dict[str, int]:
        """
        Genera embeddings para un batch de proveedores.

        Args:
            providers: Lista de proveedores
            supabase: Cliente Supabase

        Returns:
            Dict con estad√≠sticas: success, failed, total
        """
        stats = {"success": 0, "failed": 0, "total": len(providers)}

        for provider in providers:
            success = await self.generate_for_provider(provider, supabase)
            if success:
                stats["success"] += 1
            else:
                stats["failed"] += 1

        return stats

    async def generate_all(
        self,
        repository: SupabaseProviderRepository,
        test_mode: bool = False
    ) -> Dict[str, int]:
        """
        Genera embeddings para todos los proveedores verificados.

        Args:
            repository: Repositorio de proveedores
            test_mode: Si True, solo procesa 5 proveedores

        Returns:
            Dict con estad√≠sticas: success, failed, total
        """
        # Obtener proveedores verificados
        limit = 5 if test_mode else 1000

        providers = await repository.find_many(
            filters={"verified": True},
            limit=limit
        )

        if not providers:
            logger.warning("‚ö†Ô∏è No se encontraron proveedores verificados")
            return {"success": 0, "failed": 0, "total": 0}

        logger.info(f"üìã Procesando {len(providers)} proveedores verificados...")

        # Inicializar Supabase
        supabase = get_supabase_client()

        # Generar embeddings
        stats = await self.generate_batch(providers, supabase)

        logger.info(f"‚úÖ Embeddings generados: {stats['success']}/{stats['total']}")
        logger.info(f"‚ùå Fallidos: {stats['failed']}/{stats['total']}")

        # Imprimir m√©tricas
        embedding_stats = metrics.get_stats("embedding_generation")
        if embedding_stats:
            logger.info(f"üìä M√©tricas de embedding_generation:")
            logger.info(f"   - Avg: {embedding_stats.get('avg_ms', 'N/A')}ms")
            logger.info(f"   - P95: {embedding_stats.get('p95_ms', 'N/A')}ms")

        return stats


async def main():
    """Funci√≥n principal."""
    parser = argparse.ArgumentParser(
        description='Generar embeddings para proveedores',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos:
  # Generar para todos los proveedores (HF API)
  python scripts/generate_embeddings.py

  # Generar para 5 proveedores de prueba
  python scripts/generate_embeddings.py --test

  # Usar modelo local
  python scripts/generate_embeddings.py --local
        """
    )

    parser.add_argument(
        '--local',
        action='store_true',
        help='Usar modelo local (sentence-transformers) en lugar de HF API'
    )
    parser.add_argument(
        '--test',
        action='store_true',
        help='Modo prueba: solo 5 proveedores'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='Tama√±o de batch para HF API (default: 10)'
    )

    args = parser.parse_args()

    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    logger.info("="*70)
    logger.info("GENERADOR DE EMBEDDINGS - PROVEEDORES")
    logger.info("="*70)
    logger.info(f"Modo: {'LOCAL' if args.local else 'HF INFERENCE API'}")
    logger.info(f"Test: {'S√ç (5 providers)' if args.test else 'NO (todos)'}")
    logger.info("="*70)

    # Inicializar
    try:
        supabase = get_supabase_client()
        repository = SupabaseProviderRepository(supabase)
        generator = EmbeddingGenerator(use_local=args.local, batch_size=args.batch_size)

        # Generar embeddings
        stats = await generator.generate_all(repository, test_mode=args.test)

        # Resumen
        print(f"\n{'='*70}")
        print(f"üìä RESUMEN:")
        print(f"  ‚úÖ Exitosos: {stats['success']}")
        print(f"  ‚ùå Fallidos: {stats['failed']}")
        print(f"  üìã Total: {stats['total']}")
        print(f"{'='*70}\n")

        if stats['failed'] > 0:
            logger.warning(f"‚ö†Ô∏è {stats['failed']} embeddings fallaron, revisar logs")

    except Exception as e:
        logger.error(f"‚ùå Error fatal: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
```

**Dependencias a agregar:**
```bash
# Agregar a requirements.txt
sentence-transformers==2.2.2
numpy==1.24.3
httpx==0.24.0
```

---

### Fase 3: Servicio de Embeddings (D√≠a 5)

**Archivo**: `python-services/ai-clientes/services/embedding_service.py` (NUEVO)

**Integraci√≥n con arquitectura existente:**
- ‚úÖ Usa CacheManager para cachear embeddings de queries comunes
- ‚úÖ Usa PerformanceMetrics para tracking de latencias
- ‚úÖ Feature flag para activaci√≥n gradual (`USE_SEMANTIC_SEARCH`)
- ‚úÖ Fallback local si HF API falla

```python
"""
Servicio de embeddings para b√∫squeda sem√°ntica.

Integraci√≥n con arquitectura SOLID:
- CacheManager: Cach√© de embeddings de queries comunes
- PerformanceMetrics: Tracking de latencias (p50, p95, p99)
- Feature Flags: Activaci√≥n gradual (USE_SEMANTIC_SEARCH)
- Fallback: Modelo local si HF API falla

Author: Claude Sonnet 4.5
Created: 2025-01-14
"""

import os
import logging
from typing import Optional, List
import httpx
import numpy as np

from core.cache import CacheManager, CacheNamespace
from core.metrics import metrics
from core.feature_flags import get_phase_status

logger = logging.getLogger(__name__)


class EmbeddingService:
    """
    Servicio para generar embeddings via HF Inference API.

    Caracter√≠sticas:
    - Cach√© de embeddings comunes (Redis, TTL 1 hora)
    - Fallback a modelo local si HF API falla
    - Tracking autom√°tico de m√©tricas
    - Feature flag para activaci√≥n gradual

    Atributos:
        cache_manager: Instancia de CacheManager
        hf_token: Token de Hugging Face
        api_url: URL de HF Inference API
        _local_model: Modelo local (fallback)
        _enabled: Si el servicio est√° habilitado
    """

    # Configuraci√≥n
    MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
    EMBEDDING_DIMS = 384
    EMBEDDING_CACHE_TTL = 3600  # 1 hora
    API_URL = f"https://api-inference.huggingface.co/models/{MODEL_NAME}"

    def __init__(self, cache_manager: CacheManager, enable_validation: bool = False):
        """
        Inicializa el servicio.

        Args:
            cache_manager: Instancia de CacheManager
            enable_validation: Si True, valida embeddings antes de retornarlos
        """
        self.cache_manager = cache_manager
        self.enable_validation = enable_validation

        self.hf_token = os.getenv('HF_TOKEN')
        self._local_model = None
        self._enabled = False

        # Feature flag: verificar si est√° activada la fase 6
        try:
            self._enabled = get_phase_status(6) and bool(self.hf_token)
        except:
            self._enabled = bool(self.hf_token)

        if self._enabled:
            logger.info("‚úÖ EmbeddingService inicializado (HF API + cach√©)")
        else:
            logger.info("‚è∏Ô∏è EmbeddingService deshabilitado (configura HF_TOKEN o fase 6)")

    async def generate_embedding(
        self,
        text: str,
        use_cache: bool = True
    ) -> Optional[List[float]]:
        """
        Genera embedding para un texto.

        Args:
            text: Texto a embeddar
            use_cache: Si True, usa cach√© de embeddings

        Returns:
            Lista de floats (384 dimensiones) o None si falla
        """
        if not self._enabled:
            logger.warning("‚ö†Ô∏è EmbeddingService no est√° habilitado")
            return None

        # 1. Verificar cach√©
        if use_cache:
            cached = await self._get_from_cache(text)
            if cached is not None:
                logger.debug(f"‚úÖ Embedding cache HIT para: '{text[:50]}...'")
                return cached

        # 2. Generar embedding
        try:
            embedding = await self._generate(text)

            # 3. Validar embedding
            if self.enable_validation:
                if not self._validate_embedding(embedding):
                    logger.warning(f"‚ö†Ô∏è Embedding inv√°lido generando, reintentando...")
                    return None

            # 4. Guardar en cach√©
            if use_cache and embedding is not None:
                await self._save_to_cache(text, embedding)

            return embedding

        except Exception as e:
            logger.error(f"‚ùå Error generando embedding: {e}")
            return None

    async def _generate(self, text: str) -> List[float]:
        """
        Genera embedding (interna).

        Intenta HF API primero, luego fallback a modelo local.
        """
        # Intentar HF API
        if self.hf_token:
            try:
                return await self._generate_from_api(text)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è HF API fall√≥: {e}, usando fallback local")

        # Fallback a modelo local
        return await self._generate_local(text)

    async def _generate_from_api(self, text: str) -> List[float]:
        """Genera embedding usando HF Inference API."""
        async with metrics.timer("hf_inference_api"):
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    self.API_URL,
                    headers={"Authorization": f"Bearer {self.hf_token}"},
                    json={"inputs": text}
                )

                if response.status_code == 200:
                    embedding = np.array(response.json()[0])
                    # Normalizar
                    embedding = embedding / np.linalg.norm(embedding)
                    return embedding.tolist()
                else:
                    raise Exception(f"HF API error {response.status_code}: {response.text}")

    async def _generate_local(self, text: str) -> List[float]:
        """Genera embedding usando modelo local (fallback)."""
        if self._local_model is None:
            logger.info("üîÑ Cargando modelo local...")
            from sentence_transformers import SentenceTransformer
            self._local_model = SentenceTransformer(self.MODEL_NAME)
            logger.info("‚úÖ Modelo local cargado")

        async with metrics.timer("local_embedding_model"):
            embedding = self._local_model.encode(
                text,
                normalize_embeddings=True,
                show_progress_bar=False
            )
            return embedding.tolist()

    def _validate_embedding(self, embedding: List[float]) -> bool:
        """
        Valida que un embedding sea correcto.

        Args:
            embedding: Embedding a validar

        Returns:
            True si es v√°lido, False en caso contrario
        """
        if not isinstance(embedding, list):
            return False

        if len(embedding) != self.EMBEDDING_DIMS:
            return False

        # Verificar que no tenga NaN o Inf
        for val in embedding:
            if not isinstance(val, (int, float)) or np.isnan(val) or np.isinf(val):
                return False

        # Verificar que no sea todo ceros
        if all(abs(v) < 1e-6 for v in embedding):
            return False

        return True

    async def _get_from_cache(self, text: str) -> Optional[List[float]]:
        """Obtiene embedding del cach√©."""
        try:
            import hashlib
            cache_key = f"embedding:{hashlib.md5(text.encode()).hexdigest()}"

            cached = await self.cache_manager.get(
                namespace=CacheNamespace.SEARCH_RESULTS,
                identifier=cache_key
            )

            return cached

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error leyendo del cach√©: {e}")
            return None

    async def _save_to_cache(self, text: str, embedding: List[float]) -> None:
        """Guarda embedding en cach√©."""
        try:
            import hashlib
            cache_key = f"embedding:{hashlib.md5(text.encode()).hexdigest()}"

            await self.cache_manager.set(
                namespace=CacheNamespace.SEARCH_RESULTS,
                identifier=cache_key,
                value=embedding,
                ttl=self.EMBEDDING_CACHE_TTL
            )

            logger.debug(f"üíæ Embedding guardado en cach√©: {cache_key[:16]}...")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error guardando en cach√©: {e}")

    def get_stats(self) -> dict:
        """Obtiene estad√≠sticas del servicio."""
        stats = {
            "enabled": self._enabled,
            "cache_ttl_seconds": self.EMBEDDING_CACHE_TTL,
            "hf_api_configured": bool(self.hf_token),
            "local_model_loaded": self._local_model is not None,
        }

        # Agregar m√©tricas de HF API
        hf_stats = metrics.get_stats("hf_inference_api")
        if hf_stats:
            stats["hf_api_stats"] = hf_stats

        # Agregar m√©tricas de modelo local
        local_stats = metrics.get_stats("local_embedding_model")
        if local_stats:
            stats["local_model_stats"] = local_stats

        return stats


# Instancia global (se inicializa en main.py)
embedding_service: Optional[EmbeddingService] = None


def initialize_embedding_service(cache_manager: CacheManager) -> Optional[EmbeddingService]:
    """
    Inicializa el servicio de embeddings.

    Args:
        cache_manager: Instancia de CacheManager

    Returns:
        Instancia de EmbeddingService o None si hay error
    """
    global embedding_service

    try:
        if cache_manager:
            embedding_service = EmbeddingService(cache_manager)
            logger.info("‚úÖ EmbeddingService inicializado correctamente")
            return embedding_service
    except Exception as e:
        logger.error(f"‚ùå Error inicializando EmbeddingService: {e}")
        return None
```

---

### Fase 4: B√∫squeda Sem√°ntica (D√≠a 6-7)

**Archivo**: `python-services/ai-clientes/services/search_service.py` (MODIFICAR - AGREGAR AL FINAL)

**IMPORTANTE**: NO reemplaza `intelligent_search_providers()` existente. Solo agrega funciones nuevas.

```python
# =============================================================================
# B√öSQUEDA SEM√ÅNTICA (Fase 6 - Feature Flag)
# =============================================================================

# Imports adicionales
from core.feature_flags import get_phase_status
from core.metrics import metrics

# Feature flag para b√∫squeda sem√°ntica
USE_SEMANTIC_SEARCH = os.getenv("USE_SEMANTIC_SEARCH", "false") == "true"


async def semantic_search_providers(
    payload: Dict[str, Any],
    openai_semaphore: Any = None,
    OPENAI_TIMEOUT_SECONDS: int = 5
) -> Dict[str, Any]:
    """
    B√∫squeda sem√°ntica de proveedores usando embeddings.

    ENFOQUE (Fase 6):
    1. QueryInterpreterService interpreta la query con IA
    2. EmbeddingService genera embedding del query
    3. PostgreSQL + pgvector busca por similitud coseno
    4. Resultados ordenados por similitud

    Args:
        payload: Dict con main_profession, location, actual_need
        openai_semaphore: Semaphore para OpenAI (existente)
        OPENAI_TIMEOUT_SECONDS: Timeout para OpenAI (existente)

    Returns:
        Dict con providers, total, query_interpretation, search_metadata
    """
    # Verificar feature flag
    if not USE_SEMANTIC_SEARCH:
        logger.info("‚ö†Ô∏è Semantic search deshabilitado, usando b√∫squeda actual")
        return await intelligent_search_providers(
            payload,
            openai_semaphore,
            OPENAI_TIMEOUT_SECONDS
        )

    # Verificar fase 6
    phase_6_active = False
    try:
        phase_6_active = get_phase_status(6)
    except:
        phase_6_active = False

    if not phase_6_active:
        logger.warning("‚ö†Ô∏è Fase 6 no est√° activa, usando b√∫squeda actual")
        return await intelligent_search_providers(
            payload,
            openai_semaphore,
            OPENAI_TIMEOUT_SECONDS
        )

    # Verificar que EmbeddingService est√° disponible
    from services.embedding_service import embedding_service

    if not embedding_service or not embedding_service._enabled:
        logger.warning("‚ö†Ô∏è EmbeddingService no disponible, usando b√∫squeda actual")
        return await intelligent_search_providers(
            payload,
            openai_semaphore,
            OPENAI_TIMEOUT_SECONDS
        )

    profession = payload.get("main_profession", "")
    location = payload.get("location", "")
    need_summary = payload.get("actual_need", "")

    # Construir query para IA
    if need_summary and need_summary != profession:
        query = f"{need_summary} {profession} en {location}"
    else:
        query = f"{profession} en {location}"

    logger.info(f"üîç Buscando con embeddings: query='{query}'")

    try:
        # Paso 1: IA interpreta la query (EXISTENTE - DIFERENCIADOR)
        query_interpreter_svc = _get_query_interpreter()
        if not query_interpreter_svc:
            raise Exception("QueryInterpreterService no disponible")

        interpretation = await query_interpreter_svc.interpret_query(
            user_message=query,
            city_context=location,
            semaphore=openai_semaphore,
            timeout_seconds=OPENAI_TIMEOUT_SECONDS
        )

        interpreted_profession = interpretation["profession"]
        interpreted_city = interpretation["city"] or location
        details = interpretation.get("details", "")

        logger.info(
            f"üß† IA interpret√≥: '{query}' ‚Üí "
            f"profession='{interpreted_profession}', city='{interpreted_city}'"
        )

        # Paso 2: Generar embedding del query
        query_text = f"{interpreted_profession} {details}"

        async with metrics.timer("query_embedding_generation"):
            query_embedding = await embedding_service.generate_embedding(
                text=query_text,
                use_cache=True
            )

        if query_embedding is None:
            logger.warning("‚ö†Ô∏è No se pudo generar embedding, usando b√∫squeda actual")
            return await intelligent_search_providers(
                payload,
                openai_semaphore,
                OPENAI_TIMEOUT_SECONDS
            )

        logger.info(f"‚úÖ Embedding generado: {len(query_embedding)} dimensiones")

        # Paso 3: Buscar por similitud en PostgreSQL + pgvector
        provider_repo = _get_provider_repository()
        if not provider_repo:
            raise Exception("ProviderRepository no disponible")

        from app.dependencies import get_supabase_client
        supabase = get_supabase_client()

        # Convertir embedding a string vector para pgvector
        embedding_str = str(query_embedding)

        async with metrics.timer("semantic_search_db"):
            result = supabase.rpc('match_providers_semantic', {
                'query_embedding': embedding_str,
                'target_city': interpreted_city,
                'max_results': 10,
                'min_similarity': 0.5
            }).execute()

        providers = result.data if hasattr(result, 'data') else []
        total = len(providers)

        logger.info(f"‚úÖ B√∫squeda sem√°ntica: {total} proveedores encontrados")

        # Extraer scores de similitud
        similarity_scores = [p.get('similarity', 0) for p in providers[:5]]

        return {
            "ok": True,
            "providers": providers,
            "total": total,
            "query_interpretation": {
                "profession": interpreted_profession,
                "city": interpreted_city,
                "details": details
            },
            "search_metadata": {
                "strategy": "semantic_search_embeddings",
                "ai_enhanced": True,
                "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2",
                "similarity_scores": similarity_scores,
                "search_phase": "6"
            }
        }

    except Exception as exc:
        logger.error(f"‚ùå Error en b√∫squeda sem√°ntica: {exc}")
        logger.info("üîÑ Fallback a b√∫squeda actual")

        # Fallback a b√∫squeda actual
        return await intelligent_search_providers(
            payload,
            openai_semaphore,
            OPENAI_TIMEOUT_SECONDS
        )


# Wrapper para elegir estrategia de b√∫squeda (BACKWARD COMPATIBLE)
async def intelligent_search_providers_v2(
    payload: Dict[str, Any],
    openai_semaphore: Any = None,
    OPENAI_TIMEOUT_SECONDS: int = 5
) -> Dict[str, Any]:
    """
    B√∫squeda inteligente con soporte para b√∫squeda sem√°ntica (v2).

    Elige autom√°ticamente entre b√∫squeda actual vs sem√°ntica seg√∫n:
    1. Feature flag USE_SEMANTIC_SEARCH
    2. Disponibilidad de EmbeddingService
    3. Configuraci√≥n de pgvector

    Args:
        payload: Dict con main_profession, location, actual_need
        openai_semaphore: Semaphore para OpenAI (existente)
        OPENAI_TIMEOUT_SECONDS: Timeout para OpenAI (existente)

    Returns:
        Dict con providers, total, query_interpretation, search_metadata

    BACKWARD COMPATIBLE: Si semantic search falla, usa b√∫squeda actual.
    """
    if USE_SEMANTIC_SEARCH:
        try:
            return await semantic_search_providers(
                payload,
                openai_semaphore,
                OPENAI_TIMEOUT_SECONDS
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Semantic search fall√≥: {e}, usando b√∫squeda actual")

    # B√∫squeda actual (implementaci√≥n original - EXISTENTE)
    return await intelligent_search_providers(
        payload,
        openai_semaphore,
        OPENAI_TIMEOUT_SECONDS
    )


# Alias para compatibilidad con llamadas existentes
intelligent_search_providers_v2_remote = intelligent_search_providers_v2
```

---

### Fase 5: Testing y Deployment (D√≠a 8-10)

#### Tests Unitarios

**Archivo**: `python-services/ai-clientes/tests/unit/test_semantic_search.py` (NUEVO)

```python
"""
Tests unitarios para b√∫squeda sem√°ntica.
"""

import pytest
from unittest.mock import Mock, AsyncMock, patch
from services.embedding_service import EmbeddingService
from services.search_service import semantic_search_providers
from core.cache import CacheManager


class TestEmbeddingService:
    """Tests para EmbeddingService."""

    @pytest.mark.asyncio
    async def test_generate_embedding_with_cache_hit(self):
        """Test generaci√≥n de embedding con cach√© hit."""
        cache = Mock(spec=CacheManager)
        cache.get = AsyncMock(return_value=[0.1, 0.2, 0.3])

        service = EmbeddingService(cache, enable_validation=False)
        service._enabled = True

        result = await service.generate_embedding("test query")

        assert result == [0.1, 0.2, 0.3]
        cache.get.assert_called_once()

    @pytest.mark.asyncio
    async def test_generate_embedding_validation(self):
        """Test validaci√≥n de embeddings."""
        cache = Mock(spec=CacheManager)
        cache.get = AsyncMock(return_value=None)
        cache.set = AsyncMock()

        service = EmbeddingService(cache, enable_validation=True)
        service._enabled = True
        service.hf_token = "test_token"

        with patch('httpx.AsyncClient.post') as mock_post:
            mock_post.return_value = Mock(
                status_code=200,
                json=lambda: [[0.1, 0.2, 0.3]]
            )

            result = await service.generate_embedding("test")

            assert result == [0.1, 0.2, 0.3]
            cache.set.assert_called_once()


class TestSemanticSearch:
    """Tests para b√∫squeda sem√°ntica."""

    @pytest.mark.asyncio
    async def test_semantic_search_fallback(self):
        """Test fallback a b√∫squeda actual."""
        payload = {
            "main_profession": "plomero",
            "location": "Quito"
        }

        # Mock semantic search falla
        with patch('services.search_service.USE_SEMANTIC_SEARCH', True):
            with patch('services.search_service.semantic_search_providers') as mock_semantic:
                mock_semantic.side_effect = Exception("Semantic error")

                # Mock b√∫squeda actual
                with patch('services.search_service.intelligent_search_providers') as mock_current:
                    mock_current.return_value = {"ok": True, "providers": []}

                    result = await semantic_search_providers(payload)

                    # Verify: se llam√≥ a b√∫squeda actual (fallback)
                    assert mock_current.called
                    assert result["ok"] is True
```

#### Tests Manuales

```bash
# 1. Test de migraci√≥n SQL
psql $DATABASE_URL -f migrations/add_semantic_search.sql

# 2. Test de generaci√≥n de embeddings (modo test)
cd python-services/ai-proveedores
python scripts/generate_embeddings.py --test --local

# 3. Test de b√∫squeda sem√°ntica (necesita contenedor corriendo)
curl -X POST "http://localhost:8001/handle-whatsapp-message" \
  -H "Content-Type: application/json" \
  -d '{
    "from_number": "+59399123456",
    "content": "tengo goteras en el techo",
    "message_type": "text"
  }'
```

**Queries de prueba cr√≠ticas:**
- "tengo goteras" ‚Üí debe encontrar plomeros
- "cortocircuito" ‚Üí electricistas
- "necesito redecorar" ‚Üí dise√±adores de interiores
- "mi perro est√° enfermo" ‚Üí veterinarios
- "se rompi√≥ la cerca" ‚Üí carpinteros/alba√±iles

#### Deployment Gradual

1. **Feature flags en `core/feature_flags.py`:**
```python
# Fase 6: Semantic Search (NUEVA)
USE_SEMANTIC_SEARCH = os.getenv('USE_SEMANTIC_SEARCH', 'false') == 'true'
```

2. **Variables de entorno:**
```bash
# .env
HF_TOKEN=hf_xxxxxxxxxxxxxx
USE_SEMANTIC_SEARCH=false  # Activar gradualmente
```

3. **Rollback plan:**
```bash
# Si algo falla, desactivar semantic search:
export USE_SEMANTIC_SEARCH=false
docker compose restart ai-clientes

# Vuelve a b√∫squeda actual autom√°ticamente
```

---

## M√©tricas de √âxito

### Baseline Actual (Sin Embeddings)
- Interpreta bien con OpenAI (~70% precisi√≥n)
- B√∫squeda por texto exacto (ILIKE)
- Muchos falsos negativos (no encuentra proveedores v√°lidos)
- Latencia: ~200-300ms promedio

### Objetivos Week 1 (Con Embeddings)
- **Interpretaci√≥n sem√°ntica**: >85% precision
- **Falsos negativos**: Reducir en 40%
- **Latencia**: <400ms promedio (IA + embedding + DB)
- **Cache hit rate**: >60% para queries comunes
- **Costo**: <$1/mes con HF API (con cach√©)

### M√©tricas Adicionales (Performance Metrics)
- `embedding_generation_ms`: <100ms p95
- `semantic_search_db_ms`: <150ms p95
- `cache_hit_rate_embeddings`: >60%

---

## Costos Estimados

### Hugging Face Inference API
- **Gratis**: ~1000 queries/d√≠a
- **Paid**: ~$0.0001/segundo de inferencia
- **Estimado**: $0.50/mes para 50,000 queries
- **Con 80% cach√©**: ~$0.10/mes

### Modelo Local (Fallback)
- **CPU**: 0.5-1 core por query
- **Memoria**: ~500MB por modelo cargado
- **Costo**: $0 (usa infraestructura existente)

### Conclusi√≥n: Muy rentable

---

## Riesgos y Mitigaci√≥n

| Riesgo | Probabilidad | Impacto | Mitigaci√≥n |
|--------|-------------|---------|------------|
| HF API downtime | Media | Alto | ‚úÖ Fallback a modelo local implementado |
| Latencia alta | Baja | Medio | ‚úÖ Redis cach√© de embeddings (1h TTL) |
| Embeddings de mala calidad | Baja | Alto | ‚úÖ Validaci√≥n de embeddings + quality checks |
| pgvector lento | Baja | Medio | ‚úÖ √çndice HNSW + tuning de par√°metros |
| Breaking changes | Baja | Alto | ‚úÖ Feature flags + fallback a b√∫squeda actual |

---

## Resumen de Implementaci√≥n

**Tiempo total**: 8-10 d√≠as (ajustado por arquitectura existente)

**D√≠a 0.5**: Verificaci√≥n de arquitectura (pgvector, dependencias)
**D√≠a 1-2**: Setup DB (migraci√≥n SQL, tablas, √≠ndices)
**D√≠a 3-4**: Generar embeddings para proveedores existentes
**D√≠a 5**: Implementar EmbeddingService con Cache/Metrics
**D√≠a 6-7**: Implementar b√∫squeda sem√°ntica con fallback
**D√≠a 8-9**: Integraci√≥n con ai-clientes + testing
**D√≠a 10**: Deployment gradual + monitoreo

**Recurso humano**: 1 developer full-time

**Stack t√©cnico (existente + nuevo):**
- ‚úÖ sentence-transformers (NUEVO)
- ‚úÖ pgvector (NUEVO)
- ‚úÖ Hugging Face (NUEVO)
- ‚úÖ Repository Pattern (EXISTENTE)
- ‚úÖ CacheManager (EXISTENTE)
- ‚úÖ PerformanceMetrics (EXISTENTE)
- ‚úÖ Docker (EXISTENTE)

---

## Ventajas de Arquitectura Actual

**Comparado con plan original:**

| Aspecto | Plan Original | Arquitectura Actual | Ventaja |
|---------|---------------|---------------------|----------|
| **SPOF** | ai-search externo | Repository directo a Supabase | ‚úÖ M√°s confiable |
| **Cach√©** | Propuesto | ‚úÖ CacheManager completo | ‚úÖ Ya implementado |
| **M√©tricas** | Mencionado | ‚úÖ PerformanceMetrics completo | ‚úÖ M√°s detallado |
| **Feature Flags** | No mencionado | ‚úÖ Sistema completo de flags | ‚úÖ Rollback f√°cil |
| **Fallback** | B√°sico | ‚úÖ M√∫ltiples niveles de fallback | ‚úÖ M√°s robusto |

---

## Siguientes Pasos Inmediatos

### 1. ‚úÖ Verificar Pre-requisitos
```bash
# Verificar pgvector en Supabase
psql $DATABASE_URL -c "SELECT extname FROM pg_extension WHERE extname = 'vector';"

# Instalar dependencias
pip install sentence-transformers numpy httpx

# Configurar HF Token
echo "HF_TOKEN=hf_xxx" >> .env
```

### 2. ‚úÖ Crear Tablas de BD
```bash
cd python-services/ai-proveedores
psql $DATABASE_URL -f migrations/add_semantic_search.sql
```

### 3. ‚úÖ Generar Embeddings (Test)
```bash
python scripts/generate_embeddings.py --test --local
```

### 4. ‚úÖ Implementar EmbeddingService
```bash
# Crear archivo
touch ai-clientes/services/embedding_service.py

# Copiar c√≥digo de Fase 3
```

### 5. ‚úÖ Actualizar SearchService
```bash
# Agregar funciones de b√∫squeda sem√°ntica al final
# Ver Fase 4 para c√≥digo completo
```

### 6. ‚úÖ Testing
```bash
# Tests unitarios
pytest tests/unit/test_semantic_search.py -v

# Tests manuales con curl
# Ver Fase 5 para ejemplos
```

### 7. ‚úÖ Deployment Gradual
```bash
# Activar feature flag
export USE_SEMANTIC_SEARCH=true

# Reconstruir contenedor
docker compose up -d --build ai-clientes

# Verificar
curl http://localhost:8001/debug/feature-flags
```

---

## Conclusi√≥n

El plan de b√∫squeda sem√°ntica est√° **dise√±ado para extender** la arquitectura actual sin breaking changes. Los patrones SOLID implementados (Repository, Cache, Metrics, Feature Flags) facilitan enormemente la implementaci√≥n:

‚úÖ **Repository Pattern**: Acceso a datos ya abstra√≠do
‚úÖ **CacheManager**: Cach√© Redis listo para usar
‚úÖ **PerformanceMetrics**: Tracking autom√°tico de latencias
‚úÖ **Feature Flags**: Activaci√≥n gradual sin riesgo
‚úÖ **Fallback robusto**: M√∫ltiples niveles de fallback

**¬øListo para comenzar la implementaci√≥n?**

---

## Referencias

- **Sentence Transformers**: https://www.sbert.net/
- **Hugging Face Inference API**: https://huggingface.co/inference-api
- **pgvector**: https://github.com/pgvector/pgvector
- **Arquitectura SOLID**: Ver commits 9ada3ca, 5df8d85, 13a576d
